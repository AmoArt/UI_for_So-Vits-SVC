{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JkLnWQCC_pr",
        "outputId": "cb4cae2d-6e23-4c0a-c431-c827f68d4255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting google drive\n",
            "Mounted at /gdrive\n",
            "Installing dependencies\n",
            "rm: cannot remove 'datasets': No such file or directory\n",
            "Cloning into 'datasets'...\n",
            "remote: Enumerating objects: 35, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 35 (delta 16), reused 29 (delta 10), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (35/35), 11.81 KiB | 806.00 KiB/s, done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.9/dist-packages (from -r datasets/requirements.txt (line 1)) (0.0.1)\n",
            "Collecting lark\n",
            "  Downloading lark-1.1.5-py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting EbookLib\n",
            "  Downloading EbookLib-0.18.tar.gz (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.5/115.5 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from bs4->-r datasets/requirements.txt (line 1)) (4.11.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from EbookLib->-r datasets/requirements.txt (line 3)) (4.9.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from EbookLib->-r datasets/requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->bs4->-r datasets/requirements.txt (line 1)) (2.4)\n",
            "Building wheels for collected packages: EbookLib\n",
            "  Building wheel for EbookLib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for EbookLib: filename=EbookLib-0.18-py3-none-any.whl size=38790 sha256=b901a32da8d85101d472382d725e853d360a9616719441af3fa4c2e9480adb66\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/3a/ec/289c2f96d54695a17d260684be304d20a8d0bf50b08b75862e\n",
            "Successfully built EbookLib\n",
            "Installing collected packages: lark, EbookLib\n",
            "Successfully installed EbookLib-0.18 lark-1.1.5\n",
            "fimfarchive_path: /gdrive/MyDrive/pony-preservation-project/story-data/fimfarchive - Apr 8, 2022\n",
            "loading the fimfarchive\n",
            "loading the txt cache\n",
            "writing baseline template files to /tmp/fimfarchive-dump-template.txt\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "#@markdown # Step 1: Load the data\n",
        "\n",
        "ppp_path = \"/gdrive/MyDrive/pony-preservation-project/\" #@param {type:\"string\"}\n",
        "#@markdown If this crashes, just try running it again. \\\n",
        "#@markdown If you haven't added the pony-preservation-project folder to your drive, you need to do that first.\n",
        "#@markdown - Go here: https://drive.google.com/drive/u/2/folders/1MuM9Nb_LwnVxInIPFNvzD_hv3zOZhpwx\n",
        "#@markdown - Click the \"pony-preservation-project\" part of the path\n",
        "#@markdown - Select \"Add shortcut to Drive\"\n",
        "#@markdown - Update the ppp_path in the text field above to the shortcut location\n",
        "\n",
        "#@markdown ![Add shortcut to drive](https://u.smutty.horse/mewskbspdxa.png)\n",
        "\n",
        "import os\n",
        "if not os.path.exists('/gdrive'):\n",
        "    print('Mounting google drive')\n",
        "    from google.colab import drive\n",
        "    drive.mount('/gdrive')\n",
        "\n",
        "print('Installing dependencies')\n",
        "!rm -r datasets\n",
        "!git clone https://github.com/synthbot-anon/datasets.git\n",
        "!pip install -r datasets/requirements.txt\n",
        "from datasets.fimfarchive import Fimfarchive, TemplatedStoryString, read_chapter\n",
        "\n",
        "import glob\n",
        "fimfarchive_path_candidates = glob.glob(f'{ppp_path}/story-data/fimfarchive - *')\n",
        "assert fimfarchive_path_candidates, \"You need to add the pony-preservation-project folder to your google drive.\"\n",
        "\n",
        "fimfarchive_path_candidates.sort(key=os.path.getmtime)\n",
        "fimfarchive_path = os.path.normpath(fimfarchive_path_candidates[-1])\n",
        "print('fimfarchive_path:', fimfarchive_path)\n",
        "print('loading the fimfarchive')\n",
        "fimfarchive = Fimfarchive(fimfarchive_path)\n",
        "print('loading the txt cache')\n",
        "read_chapter(fimfarchive.chapter_texts, '9', 0)\n",
        "\n",
        "baseline_dump_template = r\"\"\"<|info|>\n",
        "title: {.title}\n",
        "author: {.author.name}\n",
        "tags: {join .tags.type \":\" .tags.name with \", \"}\n",
        "\n",
        "<|startoftext|>\n",
        "{join \"=== \" .chapters.title \" ===\\n\" chapter_text with \"\\n\" * 4}\n",
        "<|endoftext|>\"\"\"\n",
        "\n",
        "dump_template_file = f\"/tmp/fimfarchive-dump-template.txt\"\n",
        "\n",
        "if not os.path.exists(dump_template_file):\n",
        "    print('writing baseline template files to', dump_template_file)\n",
        "    with open(dump_template_file, \"w\") as template:\n",
        "        template.write(baseline_dump_template)\n",
        "\n",
        "dump_template_help_contents = r\"\"\"Any text outside of curly braces will be treated as normal text.\n",
        "\n",
        "Inside curly braces, you can add story data and text.\n",
        "\n",
        "    Title: {.title}\n",
        "    Author: {.author.name}\n",
        "    Link: {.url}\n",
        "\n",
        "Any field you see in the fimfarchive index.json file is accessible inside the\n",
        "curly braces. Just begin the field with a dot . and enter the full path to the\n",
        "field you want.\n",
        "\n",
        "    {.archive.date_updated}\n",
        "\n",
        "Inside {}, you can interleave story data and regular text.\n",
        "\n",
        "    { \"=== \" .title \" ===\" }\n",
        "\n",
        "\n",
        "\n",
        "As a shortcut, you can also use * to repeat some text.\n",
        "\n",
        "    { \"\\n\"*2 .title \"\\n\" .author.name \"\\n\"*2}\n",
        "\n",
        "There are two special fields: .tags and .chapters. They're special because They\n",
        "can contain multiple items in them. If you want to use these inside curly\n",
        "braces, you need to specify how to combine all of the items. You can do this\n",
        "with a {join}.\n",
        "\n",
        "This will put together all of the story tag names separated with a comma.\n",
        "\n",
        "    { join .tags.name with \", \"}\n",
        "\n",
        "And this will create a list, one per line, of each tag type and tag name.\n",
        "\n",
        "    {join .tags.type \": \" .tags.name with \"\\n\"}\n",
        "\n",
        "The resulting list would look something like this:\n",
        "\n",
        "    character: Rainbow Dash\n",
        "    character: Twilight Sparkle\n",
        "    genre: Adventure\n",
        "    genre: Romance\n",
        "    series: My Little Pony: Friendship is Magic\n",
        "\n",
        "Lastly, there's a special symbol chapter_text for accessing the story text,\n",
        "chapter by chapter. You can use it like any other field inside a {join}.\n",
        "\n",
        "    { join .chapters.title \"\\n\" chapter_text with \"\\n\"*4 }\n",
        "\n",
        "That's all. Try copy/pasting this template into the actual template file\n",
        "and running the cell to see what it looks like. Make sure to select the \"sample\"\n",
        "checkbox so it doesn't take too long.\n",
        "\"\"\"\n",
        "\n",
        "dump_template_help = f\"/tmp/fimfarchive-dump-template-help.txt\"\n",
        "if not os.path.exists(dump_template_help):\n",
        "  with open(dump_template_help, \"w\") as helpfile:\n",
        "    helpfile.write(dump_template_help_contents)\n",
        "\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2n-IxXXohxCT",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "date = \"date_published\"\n",
        "dash = (\"-\")\n",
        "#@markdown # Optional: Search for stories\n",
        "tag_query = \"trixie, -equestria girls, -character:main 7, -eqg, -a new generation, -crossover, -gore, -random, -human, -anthro, -character:young six, -gallus, -yona, -sky beak, -terramar, -smolder, -silverstream, -sandbar, -ocean flow, -ocellus\" #@param {type:\"string\"}\n",
        "####numerical_query = \"\\\"('date'.split(dash))\\\" \\u003C \\\"2013\\\"\" #@param {type:\"string\"}\n",
        "###numerical_query = \"\\\"({}.split(dash))[0]\\\" \\u003C \\\"2013\\\"\".format(\"date_published\") #@param {type:\"string\"}\n",
        "#numerical_query = f\"\\\"({''.join(date.split(dash))})\\\" \\u003C \\\"2013\\\"\" #@param {type:\"string\"}\n",
        "\n",
        "numerical_query = \"{\\\"lower\\\": \\\"2011-01-01\\\", \\\"upper\\\": \\\"2011-12-31\\\"}\" #@param {type:\"string\"}\n",
        "\n",
        "numerical_query = json.loads(numerical_query)\n",
        "lower_date = numerical_query[\"lower\"].replace(\"-\", \"\")\n",
        "upper_date = numerical_query[\"upper\"].replace(\"-\", \"\")\n",
        "numerical_query = {\"lower\": lower_date, \"upper\": upper_date}\n",
        "\n",
        "\n",
        "#numerical_query = f\"{numerical_query}\"\n",
        "\n",
        "\n",
        "tmpremove=\"\"\"\n",
        "if tag_query and numerical_query:\n",
        "  search_terms = ', '.join([tag_query.strip(), numerical_query.strip()])\n",
        "else:\n",
        "  #search_terms = tag_query or numerical_query\n",
        "  search_terms = numerical_query\n",
        "\"\"\"\n",
        "\n",
        "search_terms = ', '.join([tag_query.strip()])\n",
        "\n",
        "max_results =  5#@param {type:\"integer\"}\n",
        "\n",
        "random_sample = True #@param {type:\"boolean\"}\n",
        "add_fic_date_NONE = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### Search options:\n",
        "#@markdown - **Require multiple tags:** `twilight sparkle, celestia`. This is intuitively the same as a boolean \"AND\", and it translates logically to a set intersection. The returned results will match both requirements left and right of the comma.\n",
        "#@markdown - **Reject a tag**: `-celestia`. This is intuitively the same as a boolean \"NOT\" and it translates logically to a set complement. The returned results will not match the negated requirement.\n",
        "#@markdown - **Allow either tag:** `twilight | celestia`. This is intuitively the same as a boolean \"OR\", and it translates logically to a set union. The returned results will match either of the two requirements left or right of the comma.\n",
        "#@markdown - **Group query parts:** `twilight, -(celestia | luna)`.\n",
        "#@markdown - **Restrict to a category:** `character: non` (allowed categories: character, genre, series, content, warning). This limits the scope of a tag-based requirement to just one type of tag. If you search just `non` by itself, any story with tags `winona`, `anon`, or `non-consensual` will match. If you search `character: non`, only `winona` and `anon` will match.\n",
        "#@markdown - **Restrict by likes/dislikes/wordcount:** `.ratio * .likes > .wordcount`. Any of these restrictions must be based on some sort of comparison, which includes `>`, `<`, `>=`, `<=`, and `=`. On the left and right sides of the comparison, you can use the operations `+, -, *, /, ^, min(...), max(...)` and the special symbols `.likes, .dislikes, .wordcount, .ratio`. The arithmetic operations will follow their standard order of operations. You can group operations using parentheses. See the Notes below for details on how to access more special symbols.\n",
        "#@markdown - **Use special characters:** `\"p0n-3\"` with the surrounding double-quotes. If you use anything other than letters and numbers (e.g., a dash), you'll need to surround it with double-quotes.\n",
        "#@markdown - The `numerical_query` need to be twitten in this format _{\"lower\": \"2011-01-01\", \"upper\": \"2011-12-31\"}_ to work, with first number YYYY-MM-DD being lowet than the second, if you dont care about an speciic date search just put very low and large numbers like 1900-01-01 and 3333-12-31.\n",
        "#@markdown - The `add_fic_date_NONE` due to brokeness of fimfic, some stories fit the 'tag_query' BUT do not have any publication date atteched to themselves, so you have a choice of adding those stories in your collection, e.g. Looking for stories 2011~2012 it may grab few random ones from 2017+, BUT the more details/info you put in the tag_query the less likely the code will grab random crap\n",
        "\n",
        "#@markdown ### Notes:\n",
        "\n",
        "#@markdown - Use a `max_results` value of 0 to get all results.\n",
        "#@markdown - Be careful when searching for characters. Sometimes FimFiction uses collective tags, like `Main 6`. In these cases, a story tagged with `Main 6` may contain `Twilight Sparkle` even though it does not have a `Twilight Sparkle` tag. This search does not automatically expand tags like `Main 6` into all of their implied sub-tags.\n",
        "#@markdown - If there is no like/dislike data, both are assumed to be -1.\n",
        "#@markdown - `.ratio = .likes / .dislikes` where both values clip to 0.5. So if there are 10 likes and 0 dislikes, the ratio is treated as 20.\n",
        "#@markdown - There's no difference between the two query fields. Only stories that pass both filters will get displayed.\n",
        "#@markdown - In addition to `.likes`, `.dislikes`, `.wordcount`, and `.ratio`, you can use most fields in the fimfarchive's index.json. For example, `.author.name`, `.num_views`, and so on. The symbol name here is based on the JSON path in index.json. You can't reference per-chapter information or per-tag information using this \"dot\" notation. You also can't access information that the fimfarchive tracks inconsistently, like `.author.num_followers`, though this might get fixed if I decide to clean up the fimfarchive data.\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "def gen_results(search_terms):\n",
        "  for story_id in fimfarchive.query_stories(search_terms):\n",
        "    yield story_id\n",
        "\n",
        "#get resould + ccheck the date\n",
        "def gen_results_date(search_terms, numerical_query=None, add_fic_date_NONE=False):\n",
        "    for story_id in fimfarchive.query_stories(search_terms):\n",
        "        data = fimfarchive.stories_by_id[story_id]\n",
        "        if numerical_query is not None:\n",
        "            date_published = data[\"date_published\"]\n",
        "            if date_published is not None and numerical_query[\"lower\"] is not None:\n",
        "                lower_bound = int(numerical_query[\"lower\"])\n",
        "                date_published = date_published.split(\"T\")[0].replace(\"-\", \"\")\n",
        "                if int(date_published) < lower_bound:\n",
        "                    continue\n",
        "            if date_published is not None and numerical_query[\"upper\"] is not None:\n",
        "                upper_bound = int(numerical_query[\"upper\"])\n",
        "                date_published = date_published.split(\"T\")[0].replace(\"-\", \"\")\n",
        "                if int(date_published) > upper_bound:\n",
        "                    continue\n",
        "        elif not add_fic_date_NONE and data[\"date_published\"] is None:\n",
        "            continue\n",
        "        yield story_id\n",
        "\n",
        "def print_result(story_id):\n",
        "  data = fimfarchive.stories_by_id[story_id]\n",
        "  title = data['title']\n",
        "  tags = ', '.join([x['name'] for x in data['tags']])\n",
        "  author = data['author']['name']\n",
        "  url = data['url']\n",
        "  print(f'{title} ({data[\"completion_status\"]})')\n",
        "  print(f'author: {author}')\n",
        "  print(f'tags: {tags}')\n",
        "  print(f'{data[\"num_words\"]} words, {data[\"num_likes\"]} likes, {data[\"num_dislikes\"]} dislikes')\n",
        "  print(f'link: {url}')\n",
        "  print(f'date: {data[\"date_published\"]}')\n",
        "  print('story ID: ' + str(story_id))\n",
        "  print()\n",
        "\n",
        "#results = list(gen_results(search_terms))\n",
        "\n",
        "\n",
        "results_time = list(gen_results_date(search_terms, numerical_query, add_fic_date_NONE))\n",
        "\n",
        "results = results_time\n",
        "\n",
        "if random_sample:\n",
        "  #random.shuffle(results)\n",
        "  random.shuffle(results)\n",
        "\n",
        "if max_results > 0:\n",
        "    results = list(results)[:max_results]\n",
        "\n",
        "if random_sample and numerical_date_search:\n",
        "  random.shuffle(results_time)\n",
        "\n",
        "for id in results:\n",
        "  print_result(id)\n",
        "  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "sJzMm4xhDA7G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6df5570-151e-415b-8d41-4956efe1e4b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "warning: no match for tag pattern date\n"
          ]
        }
      ],
      "source": [
        "#@markdown # Optional: Search for tags\n",
        "\n",
        "search_terms = \"date\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Notes:\n",
        "#@markdown The search options are the same as for stories. The only difference is that this searches the set of tags on fimfiction, not stories with certain tags.\n",
        "\n",
        "results = fimfarchive.query_tags(search_terms)\n",
        "for category, tags in fimfarchive.tags_by_type.items():\n",
        "  for tag_id, tag_name in tags.items():\n",
        "    if tag_id in results:\n",
        "      tag_name = tags[tag_id]\n",
        "      print(f'{category}:{tag_name}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "nwJbK2kavIh2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "90c49855-51fe-4f88-82c4-adb519f37ccc",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "dumping stories: 100%|██████████| 91/91 [00:57<00:00,  1.59fic/s]\n",
            "writing: 100%|██████████| 91/91 [00:00<00:00, 1854.96fic/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6ac7e027-a568-4a83-9cf9-b13073e6686c\", \"fimfarchive-dump.txt\", 14330762)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "#@markdown # Download stories\n",
        "date=\"date_published\"\n",
        "dash=(\"-\")\n",
        "tag_query = \"Trixie, -equestria girls, -character:main 7, -eqg, -a new generation, -crossover, -gore, -random, -human, -anthro, -character:young six, -gallus, -yona, -sky beak, -terramar, -smolder, -silverstream, -sandbar, -ocean flow, -ocellus,  -character:sunset shimmer, -character:zecora\" #@param {type:\"string\"}\n",
        "search_terms = ', '.join([tag_query.strip()])\n",
        "\n",
        "numerical_query = \"{\\\"lower\\\": \\\"2011-01-01\\\", \\\"upper\\\": \\\"2011-12-31\\\"}\" #@param {type:\"string\"}\n",
        "\n",
        "numerical_query = json.loads(numerical_query)\n",
        "lower_date = numerical_query[\"lower\"].replace(\"-\", \"\")\n",
        "upper_date = numerical_query[\"upper\"].replace(\"-\", \"\")\n",
        "numerical_query = {\"lower\": lower_date, \"upper\": upper_date}\n",
        "\n",
        "\n",
        "\n",
        "format = \"text dump\" #@param [\"text dump\", \"zip\"]\n",
        "#@markdown - **customize text dump:** /tmp/fimfarchive-dump-template.txt\n",
        "sample = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "add_fic_date_NONE = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown **Text options**\n",
        "consistent_quotes = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### Notes:\n",
        "#@markdown - You can use the story search cell above to see which stories will be downloaded. The search options work the same way.\n",
        "#@markdown - The \"sample\" option means download the first 3 results. It's there so you can sanity check the results without being hit with a multi-gigabye download.\n",
        "#@markdown - \"consistent_quotes\" will replace fancy unicode quotes with boring ascii ones.\n",
        "#@markdown - See /tmp/fimfarchive-dump-template-help.txt for an explanation of how to format the template file.\n",
        "\n",
        "\n",
        "###\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "from joblib import Parallel, delayed\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import tarfile\n",
        "import json\n",
        "\n",
        "from datasets.fimfarchive import TemplatedStoryString\n",
        "templated_dump = TemplatedStoryString(fimfarchive, consistent_quotes)\n",
        "\n",
        "\n",
        "def gen_results_date(search_terms, numerical_query=None, add_fic_date_NONE=False):\n",
        "    for story_id in fimfarchive.query_stories(search_terms):\n",
        "        data = fimfarchive.stories_by_id[story_id]\n",
        "        if numerical_query is not None:\n",
        "            date_published = data[\"date_published\"]\n",
        "            if date_published is not None and numerical_query[\"lower\"] is not None:\n",
        "                lower_bound = int(numerical_query[\"lower\"])\n",
        "                date_published = date_published.split(\"T\")[0].replace(\"-\", \"\")\n",
        "                if int(date_published) < lower_bound:\n",
        "                    continue\n",
        "            if date_published is not None and numerical_query[\"upper\"] is not None:\n",
        "                upper_bound = int(numerical_query[\"upper\"])\n",
        "                date_published = date_published.split(\"T\")[0].replace(\"-\", \"\")\n",
        "                if int(date_published) > upper_bound:\n",
        "                    continue\n",
        "        elif not add_fic_date_NONE and data[\"date_published\"] is None:\n",
        "            continue\n",
        "        yield story_id\n",
        "\n",
        "\n",
        "results = list(gen_results_date(search_terms, numerical_query, add_fic_date_NONE))\n",
        "if sample:\n",
        "  results = itertools.islice(results, 3)\n",
        "\n",
        "results = tqdm(results, desc='dumping stories', unit='fic', position=0, leave=True)\n",
        "\n",
        "with open('/tmp/fimfarchive-dump-template.txt', encoding='utf8') as template_file:\n",
        "  template = template_file.read()\n",
        "\n",
        "os.makedirs('/tmp/fimfarchive-dump/', exist_ok=True)\n",
        "for story_id in results:\n",
        "  output_path = f'/tmp/fimfarchive-dump/{story_id}.txt'\n",
        "  try:\n",
        "    with open(output_path, 'w') as output:\n",
        "      result = templated_dump.parse(template, story_id)\n",
        "      output.write(result) \n",
        "  except KeyboardInterrupt:\n",
        "    raise\n",
        "  except:\n",
        "    print('failed to dump story', story_id)\n",
        "    os.remove(output_path)\n",
        "\n",
        "if format == 'zip':\n",
        "  with zipfile.ZipFile(\"/tmp/fimfarchive-dump.zip\", \"w\") as output:\n",
        "    for id in tqdm(results, desc='zipping', unit='fic', position=0, leave=True):\n",
        "      path = f'/tmp/fimfarchive-dump/{id}.txt'\n",
        "      if not os.path.exists(path):\n",
        "        continue\n",
        "      output.write(path, f'{id}.txt')\n",
        "  files.download('/tmp/fimfarchive-dump.zip', )\n",
        "else:\n",
        "  with open('/tmp/fimfarchive-dump.txt', 'w') as output:\n",
        "    for id in tqdm(results, desc='writing', unit='fic', position=0, leave=True):\n",
        "      input_path = f'/tmp/fimfarchive-dump/{id}.txt'\n",
        "      if not os.path.exists(input_path):\n",
        "        continue\n",
        "      with open(input_path) as input:\n",
        "        output.write(input.read())\n",
        "  files.download('/tmp/fimfarchive-dump.txt')\n",
        "\n",
        "print('Done')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}